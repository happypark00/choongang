{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"XAYy9KXXu7Xp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SemUhrm2IJ1"},"outputs":[],"source":["# clone the repository\n","%cd drive/MyDrive/ColabNotebooks\n","!rm -rf GPEN\n","!git clone https://github.com/yangxy/GPEN.git\n","%cd GPEN\n","# set up the environment \n","!pip install -r requirements.txt\n","!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3\n","# download our pre-trained models\n","!wget \"https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/RetinaFace-R50.pth?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&Expires=1961116085&Signature=GlUNW6%2B8FxvxWmE9jKIZYOOciKQ%3D\" -O weights/RetinaFace-R50.pth\n","!wget \"https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/GPEN-BFR-512.pth?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&Expires=1961116208&Signature=hBgvVvKVSNGeXqT8glG%2Bd2t2OKc%3D\" -O weights/GPEN-BFR-512.pth\n","!wget \"https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/GPEN-BFR-256.pth?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&Expires=1961116259&Signature=kMGJLSHqnvzzzqwtjUVBgngzX2s%3D\" -O weights/GPEN-BFR-256.pth\n","!wget \"https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/realesrnet_x2.pth?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&Expires=1962694780&Signature=lI%2FolhA%2FyigiTRvoDIVbtMIyhjI%3D\" -O weights/realesrnet_x2.pth\n","!wget \"https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/ParseNet-latest.pth?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&Expires=1961116134&Signature=bnMwU1JogmNbARto6G%2B7iaJQCHs%3D\" -O weights/ParseNet-latest.pth\n","!wget \"https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/model_ir_se50.pth?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&Expires=1961116170&Signature=jEyBslytwpWoh5DfKvYe2H31GgE%3D\" -O weights/model_ir_se50.pth\n"]},{"cell_type":"code","source":[],"metadata":{"id":"1zo2Y-guu6k9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget \"https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/model_ir_se50.pth?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&Expires=1961116170&Signature=jEyBslytwpWoh5DfKvYe2H31GgE%3D\" -O weights/model_ir_se50.pth\n"],"metadata":{"id":"nplF7uILgN6l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","This is a simplified training code of GPEN. It achieves comparable performance as in the paper.\n","\n","@Created by rosinality\n","\n","@Modified by yangxy (yangtao9009@gmail.com)\n","'''\n","import argparse\n","import math\n","import random\n","import os\n","import cv2\n","import glob\n","from tqdm import tqdm\n","\n","import torch\n","from torch import nn, autograd, optim\n","from torch.nn import functional as F\n","from torch.utils import data\n","import torch.distributed as dist\n","from torchvision import transforms, utils\n","\n","import __init_paths\n","from training.data_loader.dataset_face import FaceDataset\n","from face_model.gpen_model import FullGenerator, Discriminator\n","\n","from training.loss.id_loss import IDLoss\n","from distributed import (\n","    get_rank,\n","    synchronize,\n","    reduce_loss_dict,\n","    reduce_sum,\n","    get_world_size,\n",")\n","\n","from training import lpips\n","\n","\n","def data_sampler(dataset, shuffle, distributed):\n","    if distributed:\n","        return data.distributed.DistributedSampler(dataset, shuffle=shuffle)\n","\n","    if shuffle:\n","        return data.RandomSampler(dataset)\n","\n","    else:\n","        return data.SequentialSampler(dataset)\n","\n","\n","def requires_grad(model, flag=True):\n","    for p in model.parameters():\n","        p.requires_grad = flag\n","\n","\n","def accumulate(model1, model2, decay=0.999):\n","    par1 = dict(model1.named_parameters())\n","    par2 = dict(model2.named_parameters())\n","\n","    for k in par1.keys():\n","        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n","\n","\n","def sample_data(loader):\n","    while True:\n","        for batch in loader:\n","            yield batch\n","\n","\n","def d_logistic_loss(real_pred, fake_pred):\n","    real_loss = F.softplus(-real_pred)\n","    fake_loss = F.softplus(fake_pred)\n","\n","    return real_loss.mean() + fake_loss.mean()\n","\n","\n","def d_r1_loss(real_pred, real_img):\n","    grad_real, = autograd.grad(\n","        outputs=real_pred.sum(), inputs=real_img, create_graph=True\n","    )\n","    grad_penalty = grad_real.pow(2).view(grad_real.shape[0], -1).sum(1).mean()\n","\n","    return grad_penalty\n","\n","\n","def g_nonsaturating_loss(fake_pred, loss_funcs=None, fake_img=None, real_img=None, input_img=None):\n","    smooth_l1_loss, id_loss = loss_funcs\n","    \n","    loss = F.softplus(-fake_pred).mean()\n","    loss_l1 = smooth_l1_loss(fake_img, real_img)\n","    loss_id, __, __ = id_loss(fake_img, real_img, input_img)\n","    loss += 1.0*loss_l1 + 1.0*loss_id\n","\n","    return loss\n","\n","\n","def g_path_regularize(fake_img, latents, mean_path_length, decay=0.01):\n","    noise = torch.randn_like(fake_img) / math.sqrt(\n","        fake_img.shape[2] * fake_img.shape[3]\n","    )\n","    grad, = autograd.grad(\n","        outputs=(fake_img * noise).sum(), inputs=latents, create_graph=True\n","    )\n","    path_lengths = torch.sqrt(grad.pow(2).sum(2).mean(1))\n","\n","    path_mean = mean_path_length + decay * (path_lengths.mean() - mean_path_length)\n","\n","    path_penalty = (path_lengths - path_mean).pow(2).mean()\n","\n","    return path_penalty, path_mean.detach(), path_lengths\n","\n","def validation(model, lpips_func, args, device):\n","    lq_files = sorted(glob.glob(os.path.join(args.val_dir, 'lq', '*.*')))\n","    hq_files = sorted(glob.glob(os.path.join(args.val_dir, 'hq', '*.*')))\n","\n","    assert len(lq_files) == len(hq_files)\n","\n","    dist_sum = 0\n","    model.eval()\n","    for lq_f, hq_f in zip(lq_files, hq_files):\n","        img_lq = cv2.imread(lq_f, cv2.IMREAD_COLOR)\n","        img_t = torch.from_numpy(img_lq).to(device).permute(2, 0, 1).unsqueeze(0)\n","        img_t = (img_t/255.-0.5)/0.5\n","        img_t = F.interpolate(img_t, (args.size, args.size))\n","        img_t = torch.flip(img_t, [1])\n","        \n","        with torch.no_grad():\n","            img_out, __ = model(img_t)\n","        \n","            img_hq = lpips.im2tensor(lpips.load_image(hq_f)).to(device)\n","            img_hq = F.interpolate(img_hq, (args.size, args.size))\n","            dist_sum += lpips_func.forward(img_out, img_hq)\n","    \n","    return dist_sum.data/len(lq_files)\n","\n","\n","def train(args, loader, generator, discriminator, losses, g_optim, d_optim, g_ema, lpips_func, device):\n","    loader = sample_data(loader)\n","\n","    pbar = range(0, args.iter)\n","\n","    if get_rank() == 0:\n","        pbar = tqdm(pbar, initial=args.start_iter, dynamic_ncols=True, smoothing=0.01)\n","\n","    mean_path_length = 0\n","\n","    d_loss_val = 0\n","    r1_loss = torch.tensor(0.0, device=device)\n","    g_loss_val = 0\n","    path_loss = torch.tensor(0.0, device=device)\n","    path_lengths = torch.tensor(0.0, device=device)\n","    mean_path_length_avg = 0\n","    loss_dict = {}\n","\n","    if args.distributed:\n","        g_module = generator.module\n","        d_module = discriminator.module\n","\n","    else:\n","        g_module = generator\n","        d_module = discriminator\n"," \n","    accum = 0.5 ** (32 / (10 * 1000))\n","\n","    for idx in pbar:\n","        i = idx + args.start_iter\n","\n","        if i > args.iter:\n","            print('Done!')\n","\n","            break\n","\n","        degraded_img, real_img = next(loader)\n","        degraded_img = degraded_img.to(device)\n","        real_img = real_img.to(device)\n","\n","        requires_grad(generator, False)\n","        requires_grad(discriminator, True)\n","\n","        fake_img, _ = generator(degraded_img)\n","        fake_pred = discriminator(fake_img)\n","\n","        real_pred = discriminator(real_img)\n","        d_loss = d_logistic_loss(real_pred, fake_pred)\n","\n","        loss_dict['d'] = d_loss\n","        loss_dict['real_score'] = real_pred.mean()\n","        loss_dict['fake_score'] = fake_pred.mean()\n","\n","        discriminator.zero_grad()\n","        d_loss.backward()\n","        d_optim.step()\n","\n","        d_regularize = i % args.d_reg_every == 0\n","\n","        if d_regularize:\n","            real_img.requires_grad = True\n","            real_pred = discriminator(real_img)\n","            r1_loss = d_r1_loss(real_pred, real_img)\n","\n","            discriminator.zero_grad()\n","            (args.r1 / 2 * r1_loss * args.d_reg_every + 0 * real_pred[0]).backward()\n","\n","            d_optim.step()\n","\n","        loss_dict['r1'] = r1_loss\n","\n","        requires_grad(generator, True)\n","        requires_grad(discriminator, False)\n","\n","        fake_img, _ = generator(degraded_img)\n","        fake_pred = discriminator(fake_img)\n","        g_loss = g_nonsaturating_loss(fake_pred, losses, fake_img, real_img, degraded_img)\n","\n","        loss_dict['g'] = g_loss\n","\n","        generator.zero_grad()\n","        g_loss.backward()\n","        g_optim.step()\n","\n","        g_regularize = i % args.g_reg_every == 0\n","\n","        if g_regularize:\n","            path_batch_size = max(1, args.batch // args.path_batch_shrink)\n","\n","            fake_img, latents = generator(degraded_img, return_latents=True)\n","\n","            path_loss, mean_path_length, path_lengths = g_path_regularize(\n","                fake_img, latents, mean_path_length\n","            )\n","\n","            generator.zero_grad()\n","            weighted_path_loss = args.path_regularize * args.g_reg_every * path_loss\n","\n","            if args.path_batch_shrink:\n","                weighted_path_loss += 0 * fake_img[0, 0, 0, 0]\n","\n","            weighted_path_loss.backward()\n","\n","            g_optim.step()\n","\n","            mean_path_length_avg = (\n","                reduce_sum(mean_path_length).item() / get_world_size()\n","            )\n","\n","        loss_dict['path'] = path_loss\n","        loss_dict['path_length'] = path_lengths.mean()\n","\n","        accumulate(g_ema, g_module, accum)\n","\n","        loss_reduced = reduce_loss_dict(loss_dict)\n","\n","        d_loss_val = loss_reduced['d'].mean().item()\n","        g_loss_val = loss_reduced['g'].mean().item()\n","        r1_val = loss_reduced['r1'].mean().item()\n","        path_loss_val = loss_reduced['path'].mean().item()\n","        real_score_val = loss_reduced['real_score'].mean().item()\n","        fake_score_val = loss_reduced['fake_score'].mean().item()\n","        path_length_val = loss_reduced['path_length'].mean().item()\n","\n","        if get_rank() == 0:\n","            pbar.set_description(\n","                (\n","                    f'd: {d_loss_val:.4f}; g: {g_loss_val:.4f}; r1: {r1_val:.4f}; '\n","                )\n","            )\n","            \n","            if i % args.save_freq == 0:\n","                with torch.no_grad():\n","                    g_ema.eval()\n","                    sample, _ = g_ema(degraded_img)\n","                    sample = torch.cat((degraded_img, sample, real_img), 0) \n","                    utils.save_image(\n","                        sample,\n","                        f'{args.sample}/{str(i).zfill(6)}.png',\n","                        nrow=args.batch,\n","                        normalize=True,\n","                        range=(-1, 1),\n","                    )\n","\n","                lpips_value = validation(g_ema, lpips_func, args, device)\n","                print(f'{i}/{args.iter}: lpips: {lpips_value.cpu().numpy()[0][0][0][0]}')\n","\n","            if i and i % args.save_freq == 0:\n","                torch.save(\n","                    {\n","                        'g': g_module.state_dict(),\n","                        'd': d_module.state_dict(),\n","                        'g_ema': g_ema.state_dict(),\n","                        'g_optim': g_optim.state_dict(),\n","                        'd_optim': d_optim.state_dict(),\n","                    },\n","                    f'{args.ckpt}/{str(i).zfill(6)}.pth',\n","                )\n","\n","\n","if __name__ == '__main__':\n","\n","    parser = argparse.ArgumentParser()\n","\n","    parser.add_argument('--path', type=str, default='./datasets/')\n","    parser.add_argument('--base_dir', type=str, default='./')\n","    parser.add_argument('--iter', type=int, default=4000000)\n","    parser.add_argument('--batch', type=int, default=1)\n","    parser.add_argument('--size', type=int, default=256)\n","    parser.add_argument('--channel_multiplier', type=int, default=2)\n","    parser.add_argument('--narrow', type=float, default=1.0)\n","    parser.add_argument('--r1', type=float, default=10)\n","    parser.add_argument('--path_regularize', type=float, default=2)\n","    parser.add_argument('--path_batch_shrink', type=int, default=2)\n","    parser.add_argument('--d_reg_every', type=int, default=16)\n","    parser.add_argument('--g_reg_every', type=int, default=4)\n","    parser.add_argument('--save_freq', type=int, default=10000)\n","    parser.add_argument('--lr', type=float, default=0.002)\n","    parser.add_argument('--local_rank', type=int, default=0)\n","    parser.add_argument('--ckpt', type=str, default='ckpts')\n","    parser.add_argument('--pretrain', type=str, default=None)\n","    parser.add_argument('--sample', type=str, default='sample')\n","    parser.add_argument('--val_dir', type=str, default='val')\n","\n","    args = parser.parse_args('')\n","\n","    os.makedirs(args.ckpt, exist_ok=True)\n","    os.makedirs(args.sample, exist_ok=True)\n","\n","    device = 'cuda'\n","\n","    n_gpu = int(os.environ['WORLD_SIZE']) if 'WORLD_SIZE' in os.environ else 1\n","    args.distributed = n_gpu > 1\n","\n","    if args.distributed:\n","        torch.cuda.set_device(args.local_rank)\n","        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n","        synchronize()\n","\n","    args.latent = 512\n","    args.n_mlp = 8\n","\n","    args.start_iter = 0\n","\n","    generator = FullGenerator(\n","        args.size, args.latent, args.n_mlp, channel_multiplier=args.channel_multiplier, narrow=args.narrow, device=device\n","    ).to(device)\n","    discriminator = Discriminator(\n","        args.size, channel_multiplier=args.channel_multiplier, narrow=args.narrow, device=device\n","    ).to(device)\n","    g_ema = FullGenerator(\n","        args.size, args.latent, args.n_mlp, channel_multiplier=args.channel_multiplier, narrow=args.narrow, device=device\n","    ).to(device)\n","    g_ema.eval()\n","    accumulate(g_ema, generator, 0)\n","\n","    g_reg_ratio = args.g_reg_every / (args.g_reg_every + 1)\n","    d_reg_ratio = args.d_reg_every / (args.d_reg_every + 1)\n","    \n","    g_optim = optim.Adam(\n","        generator.parameters(),\n","        lr=args.lr * g_reg_ratio,\n","        betas=(0 ** g_reg_ratio, 0.99 ** g_reg_ratio),\n","    )\n","\n","    d_optim = optim.Adam(\n","        discriminator.parameters(),\n","        lr=args.lr * d_reg_ratio,\n","        betas=(0 ** d_reg_ratio, 0.99 ** d_reg_ratio),\n","    )\n","\n","    if args.pretrain is not None:\n","        print('load model:', args.pretrain)\n","        \n","        ckpt = torch.load(args.pretrain)\n","\n","        generator.load_state_dict(ckpt['g'])\n","        discriminator.load_state_dict(ckpt['d'])\n","        g_ema.load_state_dict(ckpt['g_ema'])\n","            \n","        g_optim.load_state_dict(ckpt['g_optim'])\n","        d_optim.load_state_dict(ckpt['d_optim'])\n","    \n","    smooth_l1_loss = torch.nn.SmoothL1Loss().to(device)\n","    id_loss = IDLoss(args.base_dir, device, ckpt_dict=None)\n","    lpips_func = lpips.LPIPS(net='alex',version='0.1').to(device)\n","    \n","    if args.distributed:\n","        generator = nn.parallel.DistributedDataParallel(\n","            generator,\n","            device_ids=[args.local_rank],\n","            output_device=args.local_rank,\n","            broadcast_buffers=False,\n","        )\n","\n","        discriminator = nn.parallel.DistributedDataParallel(\n","            discriminator,\n","            device_ids=[args.local_rank],\n","            output_device=args.local_rank,\n","            broadcast_buffers=False,\n","        )\n","\n","        id_loss = nn.parallel.DistributedDataParallel(\n","            id_loss,\n","            device_ids=[args.local_rank],\n","            output_device=args.local_rank,\n","            broadcast_buffers=False,\n","        )\n","\n","    dataset = FaceDataset(args.path, args.size)\n","    loader = data.DataLoader(\n","        dataset,\n","        batch_size=args.batch,\n","        sampler=data_sampler(dataset, shuffle=True, distributed=args.distributed),\n","        drop_last=True,\n","    )\n","\n","    train(args, loader, generator, discriminator, [smooth_l1_loss, id_loss], g_optim, d_optim, g_ema, lpips_func, device)\n","   \n"],"metadata":{"id":"pokuaeqjBCEz"},"execution_count":null,"outputs":[]}]}